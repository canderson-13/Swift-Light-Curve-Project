{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATAMAX_ = 10000\n",
    "\n",
    "oneday_ = 60 * 60 * 24\n",
    "\n",
    "np.random.seed(seed=120)\n",
    "\n",
    "t_start = 1e3 # first measurement time\n",
    "time_fin = 100.0 * oneday_ # last measurement time\n",
    "\n",
    "# Swift instrument dependent properties\n",
    "time_orb = 2880 # time of orbit\n",
    "count_lim = 1e-4 #  // sensitivity limit for Swift\n",
    "time_cut = 1.0 * oneday_ # ; // time after which only fractional exposure\n",
    "frac_expo = 0.1 # // fraction of exposure\n",
    "bincountsbase = 20 #; //counts per bin for 1 ct / s\n",
    "onecount = 3.8e-11 #; // one count is this much erg cm-2\n",
    "deltanu = 2.38e18 #; // 0.3 - 10 KeV - translated to Hertz\n",
    "\n",
    "################################################################################\n",
    "# an example function, using some global variables\n",
    "\n",
    "def F(t):\n",
    "  scale = 0.01\n",
    "  \n",
    "  if (t < t_break):\n",
    "    return scale * (t / t_break)**(alpha_1)\n",
    "  elif (t < t_break2):\n",
    "    return scale * (t / t_break)**(alpha_2)\n",
    "  else:\n",
    "    return scale * (t/t_break2)**(alpha_3)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def makedata(time, rate, synth, err, synth_err):\n",
    "  i = 0\n",
    "  time[0] = t_start\n",
    "  time_cur = t_start\n",
    "\n",
    "  # This while loop is basically C-programming, not python. It is therefore very\n",
    "  # slow, and might end up an unacceptable bottle-neck if many light curves\n",
    "  # are generated\n",
    "  while True:\n",
    "    n_local = i\n",
    "    rate[i] = F(time[i])\n",
    "     \n",
    "    # set time of next datapoint, rought estimate by jumping forward in time\n",
    "    # assuming current rate to be representative until next time entry\n",
    "    if (time[i] < time_cut): # early time, full exposure\n",
    "      time[i + 1] = time[i] + bincountsbase / rate[i]\n",
    "    else: # late time, only fractional exposure\n",
    "      time[i + 1] = time[i] + bincountsbase / frac_expo / rate[i]\n",
    "\n",
    "    # implement orbital time loss. It increasing the jump in time, because\n",
    "    # no new photons are added during the time loss   \n",
    "    if (time[i+1] > time_cur + time_orb and time[i+1] < time_cur + 2*time_orb):\n",
    "      time[i+1] = time_cur + 2 * time_orb\n",
    "      time_cur = time[i + 1]\n",
    " \n",
    "    # check if we either have the full requested time span or the signal\n",
    "    #/ has dropped below the detection threshold. In both cases, bail out.\n",
    "    if (time[i] > time_fin):\n",
    "      break\n",
    "    if (rate[i] < count_lim):\n",
    "      if (rate[i] < 0.5 * count_lim):\n",
    "        i = i - 1\n",
    "        n_local = n_local - 1\n",
    "      break \n",
    "\n",
    "    i = i+1\n",
    "\n",
    "  # should not happen, unless your count rate function is so high that you\n",
    "  # generate way too many data points.\n",
    "  if (n_local >= DATAMAX_):\n",
    "    print (\"too many datapoints! Lower your normalization.\")\n",
    "    exit()\n",
    "\n",
    "  # set the errors on the data points. You can experiment with different\n",
    "  # approaches to the errors here.\n",
    "  # This is 'proper python' where we set all entries in one go\n",
    "  err[0:n_local] = np.where(rate[0:n_local] >= 1e-3, rate[0:n_local] * 0.25,\n",
    "    rate[0:n_local] * 0.25 * 2)\n",
    "  \n",
    "  #print( rate[0:n_local] )# some debugging info\n",
    "  #print err[0:n_local] # some debugging info\n",
    "  deviate = np.random.normal(size=n_local) * err[0:n_local]\n",
    "  #print deviate # some debugging info\n",
    "\n",
    "  # now create perturbed data points, using the errors as computed above\n",
    "  # on the unperturbed data points as computed above  \n",
    "  synth[0:n_local] = rate[0:n_local] + deviate\n",
    "  synth_err[0:n_local] = np.where(synth[0:n_local] >= 1e-3, \n",
    "    synth[0:n_local] * 0.25, synth[0:n_local] * 0.25 * 2)\n",
    "\n",
    "  return n_local\n",
    "\n",
    "##################\n",
    "\n",
    "# prep a light curve\n",
    "alpha_1 = -2.1 + np.random.normal(loc=0., scale=0.3)\n",
    "alpha_2 = alpha_1 + 1.5\n",
    "t_break = 0.5 * oneday_ + np.random.normal(loc = 0., scale = 0.1)\n",
    "time = np.zeros(DATAMAX_)\n",
    "rate = np.zeros(DATAMAX_)\n",
    "synth = np.zeros(DATAMAX_)\n",
    "err = np.zeros(DATAMAX_)\n",
    "synth_err = np.zeros(DATAMAX_)\n",
    "\n",
    "#n = makedata(time, rate, synth, err, synth_err)\n",
    "\n",
    "# prep another one\n",
    "\n",
    "alpha_1 = -2.1 + np.random.normal(loc=0., scale=0.1)\n",
    "t_break = 0.1 * oneday_ + np.random.normal(loc = 0., scale = 0.1)\n",
    "t_break2 = 3*oneday_\n",
    "alpha_2 = alpha_1 +2\n",
    "alpha_3 = alpha_1 - 0.5\n",
    "time2 = np.zeros(DATAMAX_)\n",
    "rate2 = np.zeros(DATAMAX_)\n",
    "synth2 = np.zeros(DATAMAX_)\n",
    "err2 = np.zeros(DATAMAX_)\n",
    "synth_err2 = np.zeros(DATAMAX_)\n",
    "\n",
    "n2 = makedata(time2, rate2, synth2, err2, synth_err2)\n",
    "\n",
    "# plot both examples\n",
    "\n",
    "plt.loglog(time, rate,'r')\n",
    "plt.errorbar(time, synth, synth_err, 0, 'r.')\n",
    "\n",
    "plt.loglog(time2, rate2, 'g')\n",
    "plt.errorbar(time2, synth2, synth_err2, 0, 'g.')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power law starting with shallow then steep, plotted like curves\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import os\n",
    "\n",
    "k=0\n",
    "for k in range(0,2000):\n",
    "    DATAMAX_ = 10000\n",
    "\n",
    "    oneday_ = 60 * 60 * 24.\n",
    "\n",
    "    t_start = 1e3 # first measurement time\n",
    "    time_fin = 100.0 * oneday_ # last measurement time\n",
    "\n",
    "    # Swift instrument dependent properties\n",
    "    time_orb = 2880 # time of orbit\n",
    "    count_lim = 1e-4 #  // sensitivity limit for Swift\n",
    "    time_cut = 1.0 * oneday_ # ; // time after which only fractional exposure\n",
    "    frac_expo = 0.1 # // fraction of exposure\n",
    "    bincountsbase = 20 #; //counts per bin for 1 ct / s\n",
    "    onecount = 3.8e-11 #; // one count is this much erg cm-2\n",
    "    deltanu = 2.38e18 #; // 0.3 - 10 KeV - translated to Hertz\n",
    "\n",
    "    ################################################################################\n",
    "    # an example function, using some global variables\n",
    "\n",
    "    alpha_1 = -2.1\n",
    "    t_break = 0.5 * oneday_\n",
    "    alpha_2 = alpha_1 -1.5\n",
    "\n",
    "    def F(t):\n",
    "      scale = 0.001\n",
    "\n",
    "      if (t < t_break):\n",
    "        return scale * (t / t_break)**(alpha_1)\n",
    "      else:\n",
    "        return scale * (t / t_break)**(alpha_2)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def makedata(time, rate, synth, err, synth_err):\n",
    "      i = 0\n",
    "      time[0] = t_start\n",
    "      time_cur = t_start\n",
    "\n",
    "      # This while loop is basically C-programming, not python. It is therefore very\n",
    "      # slow, and might end up an unacceptable bottle-neck if many light curves\n",
    "      # are generated\n",
    "      while True:\n",
    "        n_local = i\n",
    "        rate[i] = F(time[i])\n",
    "\n",
    "        # set time of next datapoint, rought estimate by jumping forward in time\n",
    "        # assuming current rate to be representative until next time entry\n",
    "        if (time[i] < time_cut): # early time, full exposure\n",
    "          time[i + 1] = time[i] + bincountsbase / rate[i]\n",
    "        else: # late time, only fractional exposure\n",
    "          time[i + 1] = time[i] + bincountsbase / frac_expo / rate[i]\n",
    "\n",
    "        # implement orbital time loss. It increasing the jump in time, because\n",
    "        # no new photons are added during the time loss   \n",
    "        if (time[i+1] > time_cur + time_orb and time[i+1] < time_cur + 2*time_orb):\n",
    "          time[i+1] = time_cur + 2 * time_orb\n",
    "          time_cur = time[i + 1]\n",
    "\n",
    "        # check if we either have the full requested time span or the signal\n",
    "        #/ has dropped below the detection threshold. In both cases, bail out.\n",
    "        if (time[i] > time_fin):\n",
    "          break\n",
    "        if (rate[i] < count_lim):\n",
    "          if (rate[i] < 0.5 * count_lim):\n",
    "            i = i - 1\n",
    "            n_local = n_local - 1\n",
    "          break \n",
    "\n",
    "        i = i+1\n",
    "\n",
    "      # should not happen, unless your count rate function is so high that you\n",
    "      # generate way too many data points.\n",
    "      if (n_local >= DATAMAX_):\n",
    "        print (\"too many datapoints! Lower your normalization.\")\n",
    "        exit()\n",
    "\n",
    "      # set the errors on the data points. You can experiment with different\n",
    "      # approaches to the errors here.\n",
    "      # This is 'proper python' where we set all entries in one go\n",
    "      err[0:n_local] = np.where(rate[0:n_local] >= 1e-3, rate[0:n_local] * 0.25,\n",
    "        rate[0:n_local] * 0.25 * 2)\n",
    "\n",
    "      #print( rate[0:n_local] )# some debugging info\n",
    "      #print err[0:n_local] # some debugging info\n",
    "      deviate = np.random.normal(size=n_local) * err[0:n_local]\n",
    "      #print deviate # some debugging info\n",
    "\n",
    "      # now create perturbed data points, using the errors as computed above\n",
    "      # on the unperturbed data points as computed above  \n",
    "      synth[0:n_local] = rate[0:n_local] + deviate\n",
    "      synth_err[0:n_local] = np.where(synth[0:n_local] >= 1e-3, \n",
    "        synth[0:n_local] * 0.25, synth[0:n_local] * 0.25 * 2)\n",
    "\n",
    "      return n_local\n",
    "\n",
    "    ##################\n",
    "\n",
    "    # prep a light curve\n",
    "    alpha_1 = -1.1 + np.random.normal(loc=0., scale=0.3)\n",
    "    alpha_2 = alpha_1 - 1.5\n",
    "    t_break = 0.5 * oneday_ + np.random.normal(loc = 0., scale = 0.1)\n",
    "    time = np.zeros(DATAMAX_)\n",
    "    rate = np.zeros(DATAMAX_)\n",
    "    synth = np.zeros(DATAMAX_)\n",
    "    err = np.zeros(DATAMAX_)\n",
    "    synth_err = np.zeros(DATAMAX_)\n",
    "\n",
    "    n = makedata(time, rate, synth, err, synth_err)\n",
    "    \n",
    "    time1 = time[time!=0]\n",
    "    synth1 = synth[synth>0]\n",
    "    synth_err1 = synth_err[synth_err!=0]\n",
    "    synth_err1 = synth_err1[0:len(synth1)]\n",
    "    time1 = time1[0:len(synth1)]\n",
    "    \n",
    "    x = np.log(time1)\n",
    "    #print(len(x))\n",
    "    #print(x)\n",
    "    y = np.log(synth1)\n",
    "    #print(len(y))\n",
    "    #print(y)\n",
    "    err = synth_err1/synth1\n",
    "    #print(len(err))\n",
    "    #print(err)\n",
    "\n",
    "    #use average errors, interpolate and generate random points\n",
    "    x_avg_err = np.median(err)\n",
    "    y_avg_err = np.median(err)\n",
    "\n",
    "    #set limits on axes so the aspect ratio is a constant - no stretching\n",
    "    #constant = 1\n",
    "    tmax = x.max()\n",
    "    tmin = x.min()\n",
    "    fmin = y.min() \n",
    "    fmax = 2*(tmax-tmin)+fmin\n",
    "\n",
    "    if len(x)>4:    \n",
    "        xnew = np.linspace(min(x), max(x), num=1000, endpoint=True)\n",
    "        f2 = interp1d(x, y, kind='linear')\n",
    "        ynew = f2(xnew)\n",
    "        xnp = np.array(xnew)\n",
    "        ynp = np.array(ynew)\n",
    "\n",
    "        xpoints = []\n",
    "        ypoints = []\n",
    "        for i in range(len(xnp)):\n",
    "            xpoints.append(np.random.normal(loc=xnp[i],scale=x_avg_err, size=(10000)))\n",
    "            ypoints.append(np.random.normal(loc=ynp[i],scale=y_avg_err, size=(10000)))\n",
    "        xpoints = np.array(xpoints)\n",
    "        ypoints = np.array(ypoints)\n",
    "        xpoints=xpoints.reshape(10000000)\n",
    "        ypoints=ypoints.reshape(10000000)\n",
    "\n",
    "        #plot - setting axes limits\n",
    "        plot = plt.figure()\n",
    "        plot = plt.hist2d(xpoints, ypoints, bins=40, cmap='binary')\n",
    "        plot = plt.xlim(tmin, tmax)\n",
    "        plot = plt.ylim(fmin, fmax)\n",
    "        plot = plt.axis('off')\n",
    "        #plot = plt.show()\n",
    "        plot = plt.savefig('fake_data1/type1/l_{0}.png'.format(k))\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power law starting with steep then shallow\n",
    "#power law starting with shallow then steep, plotted like curves\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import os\n",
    "\n",
    "k=0\n",
    "for k in range(0,2000):\n",
    "    DATAMAX_ = 1000\n",
    "\n",
    "    oneday_ = 60 * 60 * 24.\n",
    "\n",
    "    t_start = 1e3 # first measurement time\n",
    "    time_fin = 100.0 * oneday_ # last measurement time\n",
    "\n",
    "    # Swift instrument dependent properties\n",
    "    time_orb = 2880 # time of orbit\n",
    "    count_lim = 1e-4 #  // sensitivity limit for Swift\n",
    "    time_cut = 1.0 * oneday_ # ; // time after which only fractional exposure\n",
    "    frac_expo = 0.1 # // fraction of exposure\n",
    "    bincountsbase = 20 #; //counts per bin for 1 ct / s\n",
    "    onecount = 3.8e-11 #; // one count is this much erg cm-2\n",
    "    deltanu = 2.38e18 #; // 0.3 - 10 KeV - translated to Hertz\n",
    "\n",
    "    ################################################################################\n",
    "    # an example function, using some global variables\n",
    "\n",
    "    alpha_1 = -2.1\n",
    "    t_break = 0.5 * oneday_\n",
    "    alpha_2 = alpha_1 -1.5\n",
    "\n",
    "    def F(t):\n",
    "      scale = 0.001\n",
    "\n",
    "      if (t < t_break):\n",
    "        return scale * (t / t_break)**(alpha_1)\n",
    "      else:\n",
    "        return scale * (t / t_break)**(alpha_2)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def makedata(time, rate, synth, err, synth_err):\n",
    "      i = 0\n",
    "      time[0] = t_start\n",
    "      time_cur = t_start\n",
    "\n",
    "      # This while loop is basically C-programming, not python. It is therefore very\n",
    "      # slow, and might end up an unacceptable bottle-neck if many light curves\n",
    "      # are generated\n",
    "      while True:\n",
    "        n_local = i\n",
    "        rate[i] = F(time[i])\n",
    "\n",
    "        # set time of next datapoint, rought estimate by jumping forward in time\n",
    "        # assuming current rate to be representative until next time entry\n",
    "        if (time[i] < time_cut): # early time, full exposure\n",
    "          time[i + 1] = time[i] + bincountsbase / rate[i]\n",
    "        else: # late time, only fractional exposure\n",
    "          time[i + 1] = time[i] + bincountsbase / frac_expo / rate[i]\n",
    "\n",
    "        # implement orbital time loss. It increasing the jump in time, because\n",
    "        # no new photons are added during the time loss   \n",
    "        if (time[i+1] > time_cur + time_orb and time[i+1] < time_cur + 2*time_orb):\n",
    "          time[i+1] = time_cur + 2 * time_orb\n",
    "          time_cur = time[i + 1]\n",
    "\n",
    "        # check if we either have the full requested time span or the signal\n",
    "        #/ has dropped below the detection threshold. In both cases, bail out.\n",
    "        if (time[i] > time_fin):\n",
    "          break\n",
    "        if (rate[i] < count_lim):\n",
    "          if (rate[i] < 0.5 * count_lim):\n",
    "            i = i - 1\n",
    "            n_local = n_local - 1\n",
    "          break \n",
    "\n",
    "        i = i+1\n",
    "\n",
    "      # should not happen, unless your count rate function is so high that you\n",
    "      # generate way too many data points.\n",
    "      if (n_local >= DATAMAX_):\n",
    "        print (\"too many datapoints! Lower your normalization.\")\n",
    "        exit()\n",
    "\n",
    "      # set the errors on the data points. You can experiment with different\n",
    "      # approaches to the errors here.\n",
    "      # This is 'proper python' where we set all entries in one go\n",
    "      err[0:n_local] = np.where(rate[0:n_local] >= 1e-3, rate[0:n_local] * 0.25,\n",
    "        rate[0:n_local] * 0.25 * 2)\n",
    "\n",
    "      #print( rate[0:n_local] )# some debugging info\n",
    "      #print err[0:n_local] # some debugging info\n",
    "      deviate = np.random.normal(size=n_local) * err[0:n_local]\n",
    "      #print deviate # some debugging info\n",
    "\n",
    "      # now create perturbed data points, using the errors as computed above\n",
    "      # on the unperturbed data points as computed above  \n",
    "      synth[0:n_local] = rate[0:n_local] + deviate\n",
    "      synth_err[0:n_local] = np.where(synth[0:n_local] >= 1e-3, \n",
    "        synth[0:n_local] * 0.25, synth[0:n_local] * 0.25 * 2)\n",
    "\n",
    "      return n_local\n",
    "\n",
    "    ##################\n",
    "\n",
    "    # prep a light curve\n",
    "    alpha_1 = -1.1 + np.random.normal(loc=0., scale=0.3)\n",
    "    alpha_2 = alpha_1 + 0.5\n",
    "    t_break = 0.5 * oneday_ + np.random.normal(loc = 0., scale = 0.1)\n",
    "    time = np.zeros(DATAMAX_)\n",
    "    rate = np.zeros(DATAMAX_)\n",
    "    synth = np.zeros(DATAMAX_)\n",
    "    err = np.zeros(DATAMAX_)\n",
    "    synth_err = np.zeros(DATAMAX_)\n",
    "\n",
    "    n = makedata(time, rate, synth, err, synth_err)\n",
    "    \n",
    "    time1 = time[time!=0]\n",
    "    synth1 = synth[synth>0]\n",
    "    synth_err1 = synth_err[synth_err!=0]\n",
    "    synth_err1 = synth_err1[0:len(synth1)]\n",
    "    time1 = time1[0:len(synth1)]\n",
    "    \n",
    "    x = np.log(time1)\n",
    "    #print(len(x))\n",
    "    #print(x)\n",
    "    y = np.log(synth1)\n",
    "    #print(len(y))\n",
    "    #print(y)\n",
    "    err = synth_err1/synth1\n",
    "    #print(len(err))\n",
    "    #print(err)\n",
    "\n",
    "    #use average errors, interpolate and generate random points\n",
    "    x_avg_err = np.median(err)\n",
    "    y_avg_err = np.median(err)\n",
    "\n",
    "    #set limits on axes so the aspect ratio is a constant - no stretching\n",
    "    #constant = 1\n",
    "    tmax = x.max()\n",
    "    tmin = x.min()\n",
    "    fmin = y.min() \n",
    "    fmax = 2*(tmax-tmin)+fmin\n",
    "\n",
    "    if len(x)>4:    \n",
    "        xnew = np.linspace(min(x), max(x), num=1000, endpoint=True)\n",
    "        f2 = interp1d(x, y, kind='linear')\n",
    "        ynew = f2(xnew)\n",
    "        xnp = np.array(xnew)\n",
    "        ynp = np.array(ynew)\n",
    "\n",
    "        xpoints = []\n",
    "        ypoints = []\n",
    "        for i in range(len(xnp)):\n",
    "            xpoints.append(np.random.normal(loc=xnp[i],scale=x_avg_err, size=(10000)))\n",
    "            ypoints.append(np.random.normal(loc=ynp[i],scale=y_avg_err, size=(10000)))\n",
    "        xpoints = np.array(xpoints)\n",
    "        ypoints = np.array(ypoints)\n",
    "        xpoints=xpoints.reshape(10000000)\n",
    "        ypoints=ypoints.reshape(10000000)\n",
    "\n",
    "        #plot - setting axes limits\n",
    "        plot = plt.figure()\n",
    "        plot = plt.hist2d(xpoints, ypoints, bins=40, cmap='binary')\n",
    "        plot = plt.xlim(tmin, tmax)\n",
    "        plot = plt.ylim(fmin, fmax)\n",
    "        plot = plt.axis('off')\n",
    "        #plot = plt.show()\n",
    "        plot = plt.savefig('fake_data1/type2/l_{0}.png'.format(k+2000))\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import os\n",
    "\n",
    "\n",
    "k=0\n",
    "for k in range(0,200):\n",
    "    DATAMAX_ = 10000000\n",
    "\n",
    "    oneday_ = 60 * 60 * 24\n",
    "\n",
    "    t_start = 1e3 # first measurement time\n",
    "    time_fin = 100.0 * oneday_ # last measurement time\n",
    "\n",
    "    # Swift instrument dependent properties\n",
    "    time_orb = 2880 # time of orbit\n",
    "    count_lim = 1e-4 #  // sensitivity limit for Swift\n",
    "    time_cut = 1.0 * oneday_ # ; // time after which only fractional exposure\n",
    "    frac_expo = 0.1 # // fraction of exposure\n",
    "    bincountsbase = 20 #; //counts per bin for 1 ct / s\n",
    "    onecount = 3.8e-11 #; // one count is this much erg cm-2\n",
    "    deltanu = 2.38e18 #; // 0.3 - 10 KeV - translated to Hertz\n",
    "\n",
    "    ################################################################################\n",
    "    # an example function, using some global variables\n",
    "\n",
    "\n",
    "    def F(t):\n",
    "        scale = 1\n",
    "\n",
    "        if (t < t_break):\n",
    "            return scale * (t / t_break)**(alpha_1)\n",
    "        elif(t < t_break2):\n",
    "            return scale * (t / t_break)**(alpha_2)\n",
    "        else:\n",
    "            scale2 = scale * (t_break2 / t_break)**(alpha_2)\n",
    "            return scale2 * (t/ t_break2)**(alpha_3)\n",
    "        \n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def makedata(time, rate, synth, err, synth_err):\n",
    "        i = 0\n",
    "        time[0] = t_start\n",
    "        time_cur = t_start\n",
    "\n",
    "          # This while loop is basically C-programming, not python. It is therefore very\n",
    "          # slow, and might end up an unacceptable bottle-neck if many light curves\n",
    "          # are generated\n",
    "        while True:\n",
    "            n_local = i\n",
    "            rate[i] = F(time[i])\n",
    "                # set time of next datapoint, rought estimate by jumping forward in time\n",
    "                # assuming current rate to be representative until next time entry\n",
    "            if (time[i] < time_cut): # early time, full exposure\n",
    "                time[i + 1] = time[i] + bincountsbase / rate[i]\n",
    "            else: # late time, only fractional exposure\n",
    "                time[i + 1] = time[i] + bincountsbase / frac_expo / rate[i]\n",
    "\n",
    "                    # implement orbital time loss. It increasing the jump in time, because\n",
    "                    # no new photons are added during the time loss   \n",
    "            if (time[i+1] > time_cur + time_orb and time[i+1] < time_cur + 2*time_orb):\n",
    "                time[i+1] = time_cur + 2 * time_orb\n",
    "                time_cur = time[i + 1]\n",
    "\n",
    "                    # check if we either have the full requested time span or the signal\n",
    "                    #/ has dropped below the detection threshold. In both cases, bail out.\n",
    "            if (time[i] > time_fin):\n",
    "                break\n",
    "            if (rate[i] < count_lim):\n",
    "                if (rate[i] < 0.5 * count_lim):\n",
    "                    i = i - 1\n",
    "                    n_local = n_local - 1\n",
    "                break \n",
    "\n",
    "            i = i+1\n",
    "\n",
    "          # should not happen, unless your count rate function is so high that you\n",
    "          # generate way too many data points.\n",
    "        if (n_local >= DATAMAX_):\n",
    "            print (\"too many datapoints! Lower your normalization.\")\n",
    "            exit()\n",
    "\n",
    "          # set the errors on the data points. You can experiment with different\n",
    "          # approaches to the errors here.\n",
    "          # This is 'proper python' where we set all entries in one go\n",
    "        err[0:n_local] = np.where(rate[0:n_local] >= 1e-3, rate[0:n_local] * 0.25,\n",
    "            rate[0:n_local] * 0.25 * 2)\n",
    "\n",
    "        np.random.seed(seed=120)\n",
    "          #print( rate[0:n_local] )# some debugging info\n",
    "          #print err[0:n_local] # some debugging info\n",
    "        deviate = np.random.normal(size=n_local) * err[0:n_local]\n",
    "          #print deviate # some debugging info\n",
    "\n",
    "          # now create perturbed data points, using the errors as computed above\n",
    "          # on the unperturbed data points as computed above  \n",
    "        synth[0:n_local] = rate[0:n_local] + deviate\n",
    "        synth_err[0:n_local] = np.where(synth[0:n_local] >= 1e-3, \n",
    "            synth[0:n_local] * 0.25, synth[0:n_local] * 0.25 * 2)\n",
    "\n",
    "        return n_local\n",
    "\n",
    "    ##################\n",
    "\n",
    "    np.random.seed(seed=None)\n",
    "    # prep a light curve\n",
    "    alpha_1 = -0.5 + np.random.uniform(low=-0.2, high=0.2)\n",
    "    t_break =  10000  + np.random.uniform(low = -2000, high = 2000)#0.2* oneday_ + np.random.normal(loc = 0., scale = 0.05)\n",
    "    t_break2 = 40000 + np.random.uniform(low = -2000, high = 2000)\n",
    "    t_break3 = 70000 + np.random.uniform(low = -2000, high = 2000)\n",
    "    alpha_2 = -3 + np.random.uniform(low=-.8, high=.8) \n",
    "    alpha_3 = -0.09 + np.random.uniform(low=-0.05, high=0.05)\n",
    "    alpha_4 = -2.3 +np.random.uniform(low=-0.5, high=0.5)\n",
    "    time = np.zeros(DATAMAX_)\n",
    "    rate = np.zeros(DATAMAX_)\n",
    "    synth = np.zeros(DATAMAX_)\n",
    "    err = np.zeros(DATAMAX_)\n",
    "    synth_err = np.zeros(DATAMAX_)\n",
    "    \n",
    "    print('alpha1 = {0}, alpha2 = {1}, tbreak = {2}'.format(alpha_1, alpha_2, t_break))\n",
    "\n",
    "    n = makedata(time, rate, synth, err, synth_err)\n",
    "\n",
    "    time1 = time[time!=0]\n",
    "    synth1 = synth[synth>0]\n",
    "    synth_err1 = synth_err[synth_err!=0]\n",
    "    synth_err1 = synth_err1[0:len(synth1)]\n",
    "    time1 = time1[0:len(synth1)]\n",
    "\n",
    "    x = np.log(time1)\n",
    "    print(len(x))\n",
    "    #print(x)\n",
    "    y = np.log(synth1)\n",
    "    #print(len(y))\n",
    "    #print(y)\n",
    "    err = synth_err1/synth1\n",
    "    #print(len(err))\n",
    "    #print(err)\n",
    "\n",
    "        #use average errors, interpolate and generate random points\n",
    "    x_avg_err = np.median(err)\n",
    "    y_avg_err = np.median(err)\n",
    "\n",
    "        #set limits on axes so the aspect ratio is a constant - no stretching\n",
    "        #constant = 1\n",
    "    if len(x)>4:\n",
    "        tmax = x.max()\n",
    "        tmin = x.min()\n",
    "        fmin = y.min() \n",
    "        fmax = 3*(tmax-tmin)+fmin\n",
    "        xnew = np.linspace(min(x), max(x), num=1000, endpoint=True)\n",
    "        f2 = interp1d(x, y, kind='linear')\n",
    "        ynew = f2(xnew)\n",
    "        xnp = np.array(xnew)\n",
    "        ynp = np.array(ynew)\n",
    "\n",
    "        xpoints = []\n",
    "        ypoints = []\n",
    "        for i in range(len(xnp)):\n",
    "            xpoints.append(np.random.normal(loc=xnp[i],scale=x_avg_err, size=(10000)))\n",
    "            ypoints.append(np.random.normal(loc=ynp[i],scale=y_avg_err, size=(10000)))\n",
    "        xpoints = np.array(xpoints)\n",
    "        ypoints = np.array(ypoints)\n",
    "        xpoints=xpoints.reshape(10000000)\n",
    "        ypoints=ypoints.reshape(10000000)\n",
    "\n",
    "            #plot - setting axes limits\n",
    "        plot = plt.figure()\n",
    "        plot = plt.hist2d(xpoints, ypoints, bins=40, cmap='binary')\n",
    "        plot = plt.xlim(tmin, tmax)\n",
    "        plot = plt.ylim(fmin, fmax)\n",
    "        plot = plt.axis('off')\n",
    "        plot = plt.show()\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
